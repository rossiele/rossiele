{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFXV4HPXq0IQ"
      },
      "source": [
        "# Generating Bash Code with Granite Code Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X6s7GSLq0IS"
      },
      "source": [
        "> **NOTE:** This recipe assumes you are working on a Linux, MacOS, or other UNIX-compatible system. While we haven't tested on Windows, some of the examples may generate valid DOS or PowerShell output. See comments below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_j32sljq0IT"
      },
      "source": [
        "## Setting Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7IKqNNuq0IT"
      },
      "source": [
        "### Python version\n",
        "\n",
        "Ensure you are running Python 3.10 or 3.11."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ssq9sO99q0IT"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 10) and sys.version_info < (3, 12), \"Use Python 3.10 or 3.11 to run this notebook.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doHhOi71q0IU"
      },
      "source": [
        "### Install dependencies\n",
        "\n",
        "Granite Kitchen comes with a bundle of dependencies that are required for notebooks. See the list of packages in its [`setup.py`](https://github.com/ibm-granite-community/granite-kitchen/blob/main/setup.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "abujA1jUq0IU",
        "outputId": "c23561ec-3ba5-44c0-89b7-92a4f6d9da8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/ibm-granite-community/utils.git\n",
            "  Cloning https://github.com/ibm-granite-community/utils.git to /tmp/pip-req-build-d1op4mq7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ibm-granite-community/utils.git /tmp/pip-req-build-d1op4mq7\n",
            "  Resolved https://github.com/ibm-granite-community/utils.git to commit 5d67648927240b208a164d2466f0dc77200450e5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.15-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain_ollama\n",
            "  Downloading langchain_ollama-0.2.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting replicate\n",
            "  Downloading replicate-1.0.4-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting python-dotenv (from ibm-granite-community-utils==0.1.dev49)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.11)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.15 (from langchain_community)\n",
            "  Downloading langchain-0.3.15-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.31 (from langchain_community)\n",
            "  Downloading langchain_core-0.3.31-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Collecting ollama<1,>=0.4.4 (from langchain_ollama)\n",
            "  Downloading ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from replicate) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from replicate) (24.2)\n",
            "Requirement already satisfied: pydantic>1.10.7 in /usr/local/lib/python3.11/dist-packages (from replicate) (2.10.5)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from replicate) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.14.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.15->langchain_community) (0.3.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain_community) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.31->langchain_community) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.3.1)\n",
            "Downloading langchain_community-0.3.15-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_ollama-0.2.2-py3-none-any.whl (18 kB)\n",
            "Downloading replicate-1.0.4-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.15-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.31-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: ibm-granite-community-utils\n",
            "  Building wheel for ibm-granite-community-utils (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-granite-community-utils: filename=ibm_granite_community_utils-0.1.dev49-py3-none-any.whl size=8536 sha256=be19cd1312ed509ebf364a32fa9d422c5291d8ce7d5a9b07a5e3b4ddde6633d5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-34ra0jz6/wheels/6c/7a/33/158ee4b8fa44fa650998d4f7cddc09aaa47b00076125aab4fc\n",
            "Successfully built ibm-granite-community-utils\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, ibm-granite-community-utils, replicate, pydantic-settings, ollama, dataclasses-json, langchain-core, langchain_ollama, langchain, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.29\n",
            "    Uninstalling langchain-core-0.3.29:\n",
            "      Successfully uninstalled langchain-core-0.3.29\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.14\n",
            "    Uninstalling langchain-0.3.14:\n",
            "      Successfully uninstalled langchain-0.3.14\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 ibm-granite-community-utils-0.1.dev49 langchain-0.3.15 langchain-core-0.3.31 langchain_community-0.3.15 langchain_ollama-0.2.2 marshmallow-3.26.0 mypy-extensions-1.0.0 ollama-0.4.7 pydantic-settings-2.7.1 python-dotenv-1.0.1 replicate-1.0.4 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "! pip install \"git+https://github.com/ibm-granite-community/utils.git\" langchain_community langchain_ollama replicate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYtduTCnq0IV"
      },
      "source": [
        "### Serving the Granite AI model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpgVSFbWq0IV"
      },
      "source": [
        "\n",
        "This notebook requires IBM Granite models to be served by an AI model runtime so that the models can be invoked or called. This notebook can use a locally accessible [Ollama](https://github.com/ollama/ollama) server to serve the models, or the [Replicate](https://replicate.com) cloud service.\n",
        "\n",
        "During the pre-work, you may have either started a local Ollama server on your computer, or setup Replicate access and obtained an [API token](https://replicate.com/account/api-tokens)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_k8ygYAq0IV"
      },
      "source": [
        "## Select your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mzdwmvcq0IV"
      },
      "source": [
        "\n",
        "Select a Granite Code model to use. Here we use a Langchain client to connect to the model. If there is a locally accessible Ollama server, we use an Ollama client to access the model. Otherwise, we use a Replicate client to access the model.\n",
        "\n",
        "When using Replicate, if the `REPLICATE_API_TOKEN` environment variable is not set, or a `REPLICATE_API_TOKEN` Colab secret is not set, then the notebook will ask for your [Replicate API token](https://replicate.com/account/api-tokens) in a dialog box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Vj0Xynwyq0IW",
        "outputId": "cbacb588-b9b0-49f3-daaa-fc4827081f34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REPLICATE_API_TOKEN not found in Google Colab secrets.\n",
            "Please enter your REPLICATE_API_TOKEN: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "from langchain_community.llms import Replicate\n",
        "from ibm_granite_community.notebook_utils import get_env_var\n",
        "\n",
        "try: # Look for a locally accessible Ollama server for the model\n",
        "    response = requests.get(os.getenv(\"OLLAMA_HOST\", \"http://127.0.0.1:11434\"))\n",
        "    model = OllamaLLM(model=\"granite-code:8b\")\n",
        "except Exception: # Use Replicate for the model\n",
        "    model = Replicate(model=\"ibm-granite/granite-8b-code-instruct-128k\",\n",
        "                      replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgz_w16oq0IW"
      },
      "source": [
        "### Detect your operating system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CenrFaoq0IW"
      },
      "source": [
        "We'll find it useful to determine the name of our operating system and use that string in queries. This is because shell commands sometimes have different options on Linux vs. MacOS, etc. We'll write our queries so they take this difference into account. We can use a helper function to determine your OS. Note that `platform.system()` returns `Windows` on Windows system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS6dyq4Wq0IW"
      },
      "source": [
        "> **TIPS:** If you are using MacOS, you can install Linux-compatible versions of many commands. Consider these two options:\n",
        "> * Install GNU Coreutils on a Mac. See [these instructions](https://superuser.com/questions/476575/replace-os-xs-shell-commands-with-the-linux-versions).\n",
        "> * Install [HomeBrew](https://brew.sh/) and use it to install Linux-compatible (and other) tools.\n",
        "\n",
        "> **Note:** The [Google Colab](https://colab.research.google.com) runtime environment is Linux."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CfQcSVo0q0IW",
        "outputId": "22d0c7e1-3570-4aa1-988c-e3a22795f0be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My OS is Linux. My shell is bash.\n"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "\n",
        "def os_name():\n",
        "    os_name = platform.system()\n",
        "    # It turns out, using \"MacOS\" is better than \"Darwin\", which is what gets returned on MacOS.\n",
        "    # For all other cases, the returned value should be fine as is, so we map the result to the desired\n",
        "    # name, but only for MacOS...\n",
        "    name_map = {'Darwin': 'MacOS'}\n",
        "    shell_map = {'Windows': 'DOS'} # On Windows and use Power Shell, change from `DOS` to `Power Shell`.\n",
        "    # ... then pass the os_name value as the second arg, which is used as the default return value.\n",
        "    # For the shell name, return `bash` by default. (You can change this to zsh, fish, etc.)\n",
        "    return name_map.get(os_name, os_name), shell_map.get(os_name, 'bash')\n",
        "\n",
        "my_os, my_shell = os_name()\n",
        "print(f\"My OS is {my_os}. My shell is {my_shell}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s7vPwD4q0IW"
      },
      "source": [
        "\n",
        "## One-shot Prompting with Granite Code\n",
        "\n",
        "In One-shot prompting, you provide the model with a question and no examples. The model will generate an answer given its training. Larger models tend to do better at this task.\n",
        "\n",
        "Note how we add additional context to the user's input prompt, such as _\"make sure you write code that works for _my_ system!\"_ (We'll see another way to do this below.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bquU-mxMtA2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qi2ShdJ4q0IW",
        "outputId": "34518809-20bd-401d-fb80-1a41ad583ed6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a bash script that should do what you're asking for:\n",
            "```\n",
            "#!/bin/bash\n",
            "# Find the first 50 files modified within the last week\n",
            "find . -type f -mtime -7 | sort -r | head -n 50 | while read file; do\n",
            " echo \"$file was last modified on $(date -r \"$file\" +\"%Y-%m-%d %H:%M:%S\")\"\n",
            "done\n",
            "```\n",
            "This script uses the `find` command to search for files in the current directory and its subdirectories that are modified within the last week (`-mtime -7`). It then sorts the results in reverse chronological order (`-r`) and selects the first 50 files (`-head -n 50`). Finally, it loops through each file and prints the file name and its last modification time using the `date` command.\n",
            "Note that this script assumes that you're running it on a Linux system. If you're using a different operating system, you may need to modify the `find` command to use the appropriate options for your system.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from textwrap import dedent\n",
        "\n",
        "prompt = dedent(f\"\"\"\\\n",
        "    Show me a {my_shell} script to print the first 50 files found under the current working directory\n",
        "    that have been modified within the last week. Make sure you show the last modification time\n",
        "    for each file in the output. Make sure you generate {my_shell} code that is {my_os}-compatible!\"\"\"\n",
        ")\n",
        "\n",
        "response = model.invoke(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtUFvSQdq0IW"
      },
      "source": [
        "### Try the script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ySR6By2q0IX"
      },
      "source": [
        "Remove any markdown formatting in the output and paste the commands generated into the next cell _**after the %%bash line shown**_. Also delete the `ls -l`, which is there to allow the cell to run without error if nothing is pasted there (e.g., in our CI/CD test system). So, for example, you might have something like the following:\n",
        "\n",
        "```shell\n",
        "%%bash\n",
        "find dir -type d | do_something\n",
        "...\n",
        "```\n",
        "\n",
        "The `%%bash` \"magic\" tells Jupyter to run the commands as a shell script instead of as Python code. You can omit lines like `#!/bin/bash` and keep or remove any comments `# this is a comment...`.\n",
        "\n",
        "Does the script work? If not try running the query again. Also try modifying the query string. What difference do these steps make?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y4BqTzt1q0IX",
        "outputId": "ebbe1b33-31fe-45d4-cf07-7240777340d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 Jan 22 14:23 sample_data\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "ls -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SFm7AO0q0IX"
      },
      "source": [
        "## Few-shot Prompting with Granite Code\n",
        "\n",
        "In few-shot prompting, you provide the model with a question and some examples. The model will generate an answer given its training. The additional examples help the model zero in on a pattern, which may be required for smaller models to perform well at this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqG16sLJq0IX"
      },
      "source": [
        "### Provide a list of Q&A examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az9bDMoqq0IX"
      },
      "source": [
        "\n",
        "One of the examples uses the `stat` command, which requires different syntax for Linux vs. MacOS systems.\n",
        "\n",
        "> **NOTE:** If you are using a Windows system, try changing the \"answers\" in the `examples` cell to be valid Power Shell or DOS commands. You can ignore the `stat_flags` in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cdMEqzCFq0IX",
        "outputId": "bde4c995-57b5-46fb-f54a-fc4de3e4f4e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 'stat' flags for my OS 'Linux' and shell 'bash' are '-c \"%y %n\" {}'\n"
          ]
        }
      ],
      "source": [
        "stat_flags = '-c \"%y %n\" {}'\n",
        "if my_os == 'MacOS':\n",
        "    stat_flags = '-f \"%m %N\" {}'\n",
        "print(f\"The 'stat' flags for my OS \\'{my_os}\\' and shell \\'{my_shell}\\' are \\'{stat_flags}\\'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mkpJgfWsq0IX"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"question\": \"Recursively find files that match '*.js', and filter out files with 'excludeddir' in their paths.\",\n",
        "        \"answer\": \"find . -name '*.js' | grep -v excludeddir\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Dump \\\"a0b\\\" as hexadecimal bytes.\",\n",
        "        \"answer\": \"printf \\\"a0b\\\" | od -tx1\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Create a tar ball of all pdf files in the current folder and any subdirectories.\",\n",
        "        \"answer\": \"find . -name '*.pdf' | xargs tar czvf pdf.tar\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Sort all files and directories in the current directory, but no subdirectories, according to modification time, and print only the seven most recently modified items.\",\n",
        "        \"answer\": f\"find . -maxdepth 1 -exec stat {stat_flags} \\; | sort -n -r | tail -n 7\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Find all the empty directories in and under the current directory.\",\n",
        "        \"answer\": \"find . -type d -empty\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYyhc8Mqq0IX"
      },
      "source": [
        "### Assemble the prompt template\n",
        "\n",
        "Here we build up a chat prompt template from messages. See the [Langchain docs](https://python.langchain.com/docs/how_to/few_shot_examples_chat/#fixed-examples) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BZTIGpxmq0IX",
        "outputId": "d1078794-7ae2-49f7-a453-76e2a9396372",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['question']\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{question}\"),\n",
        "        (\"ai\", \"{answer}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        few_shot_prompt,\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(chat_template.input_variables)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJoYA-Xrq0IX"
      },
      "source": [
        "### View the completed prompt\n",
        "\n",
        "Create a prompt and inspect the fully-interpolated chat template. Alternating Human/AI messages create a structure that the model will follow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8pEtI8uUq0IY",
        "outputId": "0c283f61-a105-4fbb-9829-bbb21155a1ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Recursively find files that match '*.js', and filter out files with 'excludeddir' in their paths.\n",
            "AI: find . -name '*.js' | grep -v excludeddir\n",
            "Human: Dump \"a0b\" as hexadecimal bytes.\n",
            "AI: printf \"a0b\" | od -tx1\n",
            "Human: Create a tar ball of all pdf files in the current folder and any subdirectories.\n",
            "AI: find . -name '*.pdf' | xargs tar czvf pdf.tar\n",
            "Human: Sort all files and directories in the current directory, but no subdirectories, according to modification time, and print only the seven most recently modified items.\n",
            "AI: find . -maxdepth 1 -exec stat -c \"%y %n\" {} \\; | sort -n -r | tail -n 7\n",
            "Human: Find all the empty directories in and under the current directory.\n",
            "AI: find . -type d -empty\n",
            "Human: Show me a bash script to print the first 50 files found under the current working directorythat have been modified within the last week. Make sure you show the last modification timefor each file in the output.\n"
          ]
        }
      ],
      "source": [
        "# Construct a prompt with no tabs and newlines.\n",
        "prompt = dedent(f\"\"\"\\\n",
        "    Show me a {my_shell} script to print the first 50 files found under the current working directory\n",
        "    that have been modified within the last week. Make sure you show the last modification time\n",
        "    for each file in the output.\"\"\"\n",
        ").replace(\"\\n\", \"\")\n",
        "\n",
        "print(chat_template.format(question=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ax5bDo0q0IY"
      },
      "source": [
        "### Run the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PGVxiKzuq0IY",
        "outputId": "beba53a7-78a6-4818-d5df-e8c07cda30d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a bash script that should do what you're asking for:\n",
            "```\n",
            "#!/bin/bash\n",
            "find . -type f -mtime -7 | head -n 50 | while read file; do\n",
            " echo \"$(stat -c %y \"$file\") $file\"\n",
            "done\n",
            "```\n",
            "This script uses the `find` command to search for files in the current directory and its subdirectories that have been modified within the last week (`-mtime -7`). It then pipes the output to `head -n 50` to limit the number of files to the first 50, and then uses a `while` loop to read each file in turn and print its last modification time using `stat -c %y`.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chain = chat_template | model\n",
        "response = chain.invoke({\"question\": prompt})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ga-vA3eq0IY"
      },
      "source": [
        "## Adding a System Prompt\n",
        "\n",
        "Finally, a _system prompt_ is the preferred way to provide additional instructions and clarity about the context for a task, especially when this same information applies for _all_ user queries in the application. When you are building an AI-enabled application for a set of use cases, you will probably spend a lot of time refining the system prompt to maximize the quality of the results!\n",
        "\n",
        "Here we define a system prompt to let the model know what we expect from it, including the expected language and os compatibility. To separate the system prompt from the user prompt, we use a chat template, which separates the prompt into messages and allows us to indicate the role of the speaker.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Gpf74ounq0IY",
        "outputId": "4f1ba6e2-ee00-4dd7-8676-22b2b5fa0467",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['question']\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "system_prompt = SystemMessage(content=dedent(f\"\"\"\\\n",
        "    You are a helpful software engineer. You write clear, concise, well-commented code. Do not output anything but the code.\n",
        "    Make sure you only generate {my_shell} code that is {my_os}-compatible!\"\"\"\n",
        "))\n",
        "\n",
        "chat_template_with_system_message = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        system_prompt,\n",
        "        few_shot_prompt,\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(chat_template_with_system_message.input_variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn-71fhxq0IY"
      },
      "source": [
        "### Run the model\n",
        "\n",
        "Note that the model has removed its chatter from around the code block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UCvxFM40q0IY",
        "outputId": "ac543c43-c7b6-415b-e3ea-2e6edf78b119",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a bash script that prints the first 50 files found under the current working directory that have been modified within the last week:\n",
            "```\n",
            "find . -type f -mtime -7 | head -n 50 | while read file; do\n",
            " echo \"$file was modified on $(date -r \"$file\" +\"%Y-%m-%d %H:%M:%S\")\"\n",
            "done\n",
            "```\n",
            "This script uses the `find` command to search for files that are modified within the last week (`-mtime -7`). The `head` command is used to limit the output to the first 50 files. The script then uses a `while` loop to iterate over each file found and print the file name and the last modification time using the `date` command.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chain_sys = chat_template_with_system_message | model\n",
        "response = chain_sys.invoke({\"question\": prompt})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weGI0z4hq0IY"
      },
      "source": [
        "### For further study\n",
        "- Try different queries. Test them to see if the model generated them correctly.\n",
        "- Try invoking the model chain several times. How do the responses change from one invocation to the next?\n",
        "- Try adding more examples to the `examples` string or modifying the ones shown. Does this affect the outputs?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import dedent\n",
        "\n",
        "prompt = dedent(f\"\"\"\\\n",
        "    Show me a SQL query the gets a sum aggregation over a group by\"\"\"\n",
        ")\n",
        "\n",
        "response = model.invoke(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "QsZ2cvmntCuC",
        "outputId": "38b0d6f6-5ef4-4392-cd34-ddad8d8b6acd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here's an example SQL query that gets a sum aggregation over a group by:\n",
            "```\n",
            "SELECT customer_name, SUM(order_total) AS total_orders\n",
            "FROM orders\n",
            "GROUP BY customer_name;\n",
            "```\n",
            "This query selects the `customer_name` column and the sum of the `order_total` column for each customer, grouped by `customer_name`. The `SUM()` function is used to calculate the sum of the `order_total` column for each group, and the `AS` keyword is used to give the resulting column an alias of `total_orders`.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}